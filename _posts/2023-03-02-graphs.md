---
title: 'Graph Attention Networks'
description: An inplementation.
featured_image: '/images/demo/demo-square.jpg'
---

Graph Networks have been around for quite a while, and there are plenty of sources online that can give you a good overview. <br>This [lesson](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial7/GNN_overview.html) from <b>UvA Deep Learning Tutorials</b> is a great start. 

But often, the implementation gets quite a handful, and as a result, people skip. ðŸ˜ž 

I tried to make a simple implementation, easy and intuitive.  
Here we get the basic Graph Convolutional Layer, and the Graph Attention Layer, <b>multi-headed</b>! Hehe! 

### Graph Convolutional Layer
For a gentle start, let's begin with a Graph Convolutional Network.

So suppose we have such a network where the adjacent nodes embeddings would be 
1. gathered, 
2. projected to another dimension
3. and passed as an average. 

That means, the embeddings are summed up, and then divided by the number of nodes where the embeddings are <b>collected from</b>.

![](/images/graphs/CodeCogsEqn.svg)

Okay, a key point to note is that the neighbors of a node <b>include itself too</b>! The idea is to let a node embedding interact with neighbors and itself. So, ðŸ™ƒ.

As a result, the forward pass of the function would need the adjacency matrix, for gathering and averaging, along with the node features.
<br>The resultant code looks like this :

```python
class GCNLayer(nn.Module) :

    def __init__(self, d_in, d_out) :
        super().__init__()
        self.projection = nn.Linear(d_in, d_out)

    def forward(self, x, adj_hat) :
        # x : Node features : batch, n_nodes, d_in
        # adj_hat : adj matrix with self connections : batch, n_nodes, n_nodes

        x = self.projection(x) # to another dimension
        x = torch.bmm(adj_hat, x) # for all node embeddings, in a matrix form
        x = x / adj_hat.sum(dim = -1, keepdims = True) # averaging

        return x 
```

And obviously, any kind of non-linearity is welcome. 

This was rather simple, right? Well, the original paper does the averaging in a slightly different way.<br>

Originally each node embedding, while summing, has to be normalized by the <b>square-root of the contributing node's neighbourhood size</b> and the <b>square-root of the resulting node's neighbourhood size</b>.

Aaaah, let's look at an example, with Adjacency matrix <i><b>A</b></i> as in the figure.

![](/images/graphs/graphs1.png)

Hmm, can this be made into a matrix operation and get it done with at once?

Coside this matrix <i><b>D</b></i>, which is a diagonal matrix with elements equal to the number of neighbours of a node.

![](/images/graphs/d.svg)

Being a diagonal matrix, inverting and raising it to some arbitrary power is easy. So now we have this, 

![](/images/graphs/d_half.svg)

And lastly, <i><b>A</b></i> with self connections would be <i><b>\hat{A} </b></i>,

![](/images/graphs/a_hat.svg)

The following operation gives, (saving you some math, but feel free to do it on your own ðŸ˜Œ)

![](/images/graphs/mul.svg)

which is the exact set of coefficients we need to calculate the next layer embeddings. ðŸ˜‡
Finally, we can reduce it to,

![](/images/graphs/equ.svg)

The code changes as :
```python
class GCNLayerOrig(nn.Module) :

    def __init__(self, d_in, d_out) :
        super().__init__()
        self.projection = nn.Linear(d_in, d_out)

    def forward(self, x, adj_hat) :
        # x : Node features : batch, n_nodes, d_in
        # adj_hat : adj matrix with self connections : batch, n_nodes, n_nodes

        n_nodes = adj_hat.size()[1]
        adj = adj_hat - torch.eye(n_nodes) # without self connections

        d_hat = adj.sum(dim = -1)
        d_hat = torch.pow(d_hat, -0.5)
        d_hat = torch.diag_embed(d_hat) # batch, n_nodes, n_nodes

        dad = torch.bmm(torch.bmm(d_hat, adj_hat), d_hat) # normalizing matrix

        x = self.projection(x) # to another dimension
        x = torch.bmm(dad, x) # for all node embeddings, in a matrix form

        return x 
```

And that's the original GCN Layer. ðŸ˜„

### Graph Attention Layer + Multi Headed

1. An equation with alphas
2. Few diagrams explaining the head thing
3. functions : interleave and repeat
4. einsum : link to outside world
5. walkthrough
 
```python
class GATLayer(nn.Module) :

    def __init__(self, d_in, d_out, n_heads = 1, concat_heads = True, alpha = 0.2) :
        super().__init__()
        
        self.n_heads = n_heads
        self.concat_heads = concat_heads
        self.d_out = d_out
        if concat_heads :
            assert d_out % n_heads == 0
            self.d_out = d_out // n_heads

        self.projection = nn.Linear(d_in, d_out * n_heads)
        self.a = nn.Parameter(torch.Tensor(n_heads, 2 * d_out))
        self.leakyRelu = nn.LeakyReLU(alpha)
        self.softmax = nn.Softmax(dim = -2)

        # from the original paper
        nn.init.xavier_uniform_(self.projection.weight.data, gain = 1.414)
        nn.init.xavier_uniform_(self.a.data, gain = 1.414)

    def forward(self, x, adj_hat, return_attentions = False) :
        # x : Node features : batch_size, n_nodes, c_in
        # adj_hat : adj matrix with self connections : batch_size, n_nodes, n_nodes

        B, N = x.size()[ : 2]

        x = self.projection(x)
        x = x.view(B, N, self.n_heads, self.d_out)

        # p.shape : B, N x N, n_heads, 2 x d_out
        p1 = x.repeat_interleave(N, dim = 1)
        p2 = x.repeat(1, N, 1, 1)
        p = torch.cat([p1, p2], dim = -1)
        p = p.view(B, N, N, self.n_heads, 2 * self.d_out)

        e = torch.einsum("bpqhd, hd -> bpqh", p, self.a)

        e = self.leakyRelu(e)

        # where there is no connection, att = 0
        e = torch.where(adj_hat.unsqueeze(-1) == 0, float("-inf"), e)

        attentions = self.softmax(e)
        res = torch.einsum("bmnh, bnhd -> bmhd", attentions, x)

        if self.concat_heads :
            res = res.reshape(B, N, self.n_heads * self.d_out)
        else :
            res = res.mean(dim = -1)

        if return_attentions :
            return res, attentions
        return res
```